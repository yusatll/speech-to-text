{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uo1vU_2CSeHs",
    "outputId": "1ed44dc9-1e1f-4506-cb12-643c587acf6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping openai-whisper as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping whisper as it is not installed.\u001b[0m\u001b[33m\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,107 kB]\n",
      "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [59.5 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,164 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,457 kB]\n",
      "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,452 kB]\n",
      "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.9 kB]\n",
      "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [54.5 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,269 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,353 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]\n",
      "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,422 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,700 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.7 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
      "Get:26 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,608 kB]\n",
      "Fetched 27.3 MB in 5s (5,963 kB/s)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "97 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 97 not upgraded.\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing installations\n",
    "# !pip uninstall -y openai-whisper whisper\n",
    "\n",
    "# Install ffmpeg\n",
    "!apt update && apt install ffmpeg\n",
    "\n",
    "# Install whisper\n",
    "!pip install -q openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OjXH0YP5SWE2"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "from google.colab import files\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ld1-ojK-S3Q-",
    "outputId": "8cf92bb7-6e99-45c4-f708-32bd7ec0563f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Bilgisi:\n",
      "Mon Nov 11 19:13:01 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   50C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Bilgisi:\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaT8V2Q8RygI",
    "outputId": "bab5dbc8-aeab-4e74-839c-7aedfcbc2c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transkripsiyon başlıyor...\n",
      "\n",
      "GPU Bilgisi:\n",
      "Mon Nov 11 19:13:21 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   50C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n",
      "Liderlik-2. Ders.m4a dosyası large model ile işleniyor...\n",
      "\n",
      "large modeli yükleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2.88G/2.88G [00:30<00:00, 101MiB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ses metne dönüştürülüyor...\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive to access files\n",
    "# drive.mount('/content/drive') \n",
    "\n",
    "def process_audio_from_drive(audio_path, model_size=\"large\", output_folder=None):\n",
    "    \"\"\"\n",
    "    Process audio file from Google Drive using Whisper model\n",
    "    \n",
    "    Args:\n",
    "        audio_path (str): Full path to the audio file in Drive\n",
    "        model_size (str): Whisper model size (\"tiny\", \"base\", \"small\", \"medium\", \"large\")\n",
    "        output_folder (str): Drive folder for output file (default: same folder as audio)\n",
    "    \n",
    "    Returns:\n",
    "        str: Transcribed text if successful, None otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Display GPU information for monitoring resources\n",
    "        print(\"\\nGPU Information:\")\n",
    "        !nvidia-smi\n",
    "        \n",
    "        # Verify file exists before processing\n",
    "        if not os.path.exists(audio_path):\n",
    "            print(f\"ERROR: {audio_path} not found!\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nProcessing {os.path.basename(audio_path)} with {model_size} model...\")\n",
    "        \n",
    "        # Load the specified Whisper model\n",
    "        print(f\"\\nLoading {model_size} model...\")\n",
    "        model = whisper.load_model(model_size)\n",
    "        \n",
    "        # Track processing time for performance monitoring\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform transcription with specified parameters\n",
    "        print(\"\\nConverting speech to text...\")\n",
    "        result = model.transcribe(\n",
    "            audio_path,\n",
    "            language=\"en\",  # Change language as needed (e.g., \"tr\" for Turkish)\n",
    "            fp16=True      # Use float16 for faster GPU processing\n",
    "        )\n",
    "        \n",
    "        # Calculate and display processing duration\n",
    "        process_time = time.time() - start_time\n",
    "        minutes = int(process_time // 60)\n",
    "        seconds = int(process_time % 60)\n",
    "        print(f\"\\nProcessing completed in {minutes} minutes {seconds} seconds\")\n",
    "        \n",
    "        # Determine output directory\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.dirname(audio_path)\n",
    "        \n",
    "        # Generate output filename with metadata\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        input_filename = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "        output_filename = f\"{input_filename}_{model_size}_{process_time:.0f}s.txt\"\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        \n",
    "        # Save transcription to file\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result[\"text\"])\n",
    "        \n",
    "        print(f\"\\nResult saved to Drive: {output_path}\")\n",
    "        \n",
    "        return result[\"text\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify path to your audio file in Drive\n",
    "    AUDIO_FILE = \"/content/drive/MyDrive/Lectures/Lecture-2.m4a\"  # Change this path\n",
    "    \n",
    "    # Optional: Specify output folder (defaults to audio file location if not specified)\n",
    "    OUTPUT_FOLDER = \"/content/drive/MyDrive/Lectures\"  # Change this path\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    \n",
    "    print(\"Starting transcription...\")\n",
    "    transcription = process_audio_from_drive(\n",
    "        audio_path=AUDIO_FILE,\n",
    "        model_size=\"large\",\n",
    "        output_folder=OUTPUT_FOLDER\n",
    "    )\n",
    "    \n",
    "    if transcription:\n",
    "        print(\"\\nTranscription completed successfully!\")\n",
    "        print(\"\\nFirst 500 characters of the text:\")\n",
    "        print(transcription[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lckiUjUuR2hW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
